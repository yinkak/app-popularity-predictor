# Comparing Proprietary vs. Open-Source Apps: Predicting App Popularity ðŸ“²

## Overview

This project compares apps from the Google Play Store (proprietary) with those from F-Droid (open source) by analyzing various features and building predictive models to gauge app popularity (using ratings and installs). The project covers all stages from data acquisition, cleaning, combining, and feature derivation to exploratory analysis and model building. The modeling phase includes both a regression task (predicting app rating) and a classification task (predicting platform membership using Random Forest variants).

Research Questions
â€¢ Popularity Prediction: Can we accurately predict app popularity (e.g., ratings) using a set of derived features?
â€¢ Platform Differences: Are there systematic differences between open-source (F-Droid) and proprietary (Google Play) apps based on attributes such as reviews, app age, size, and install counts?
â€¢ Feature Importance & Insights: Which features most strongly influence app popularity metrics?

# Project Structure

```
CMPT-353-PROJECT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â”‚   â”œâ”€â”€ fdroid_cleaned.csv         # Cleaned F-Droid data (converted from JSON)
â”‚   â”‚   â””â”€â”€ googleplay_cleaned.csv     # Cleaned Google Play data (from Kaggle)
â”‚   â”œâ”€â”€ combined/
â”‚   â”‚   â”œâ”€â”€ combined_apps.csv          # Combined dataset (raw version)
â”‚   â”‚   â””â”€â”€ combined_apps_enhanced.csv # Combined dataset with additional derived features
â”‚   â””â”€â”€ uncleaned/
â”‚       â”œâ”€â”€ fdroid.json                # Raw F-Droid JSON data scraped from F-Droid repository
â”‚       â””â”€â”€ googleplaystore.csv        # Raw Google Play data downloaded from Kaggle
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ feature_importances.png        # Barplot for top 10 feature importances (modeling)
â”‚   â”œâ”€â”€ platform_confusion_matrix.png  # Confusion matrix for platform classification
â”‚   â””â”€â”€ platform_confusion_matrix_fair.png # Alternative confusion matrix (if used)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Feature_Derivation.ipynb       # Notebook for deriving additional features
â”‚   â””â”€â”€ Visualize_Trends.ipynb         # Notebook for exploratory visualizations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ acquire_fdroid.py              # Script to download F-Droid data via API
â”‚   â”œâ”€â”€ acquire_googleplay.py          # Script to download Google Play data from Kaggle
â”‚   â”œâ”€â”€ clean_fdroid.py                # Script to clean and process F-Droid data
â”‚   â”œâ”€â”€ clean_googleplay.py            # Script to clean and process Google Play data
â”‚   â”œâ”€â”€ combine_datasets.py            # Script to merge FDroid and Google Play datasets
â”‚   â”œâ”€â”€ build_model.py                 # Script to build and evaluate the regression model
â”‚   â””â”€â”€ platform_classifier_RandomForests.py  # Script for platform classification using Random Forests
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ .gitignore                         # Files and directories to ignore in Git
â””â”€â”€ requirements.txt                   # Python package dependencies

```

## Required Libraries

The project is implemented in Python 3.x. The main packages required include:

â€¢ pandas â€“ Data manipulation and cleaning

â€¢ numpy â€“ Numerical operations

â€¢ matplotlib and seaborn â€“ Data visualization

â€¢ scikit-learn â€“ Model building, preprocessing, and evaluation

â€¢ requests â€“ HTTP requests for data acquisition

â€¢ kaggle â€“ To interact with the Kaggle API for data downloads

â€¢ jupyter â€“ For running the notebooks

Install the libraries using:

```bash
pip install -r requirements.txt
```

## Setup and Execution

As the files required to run the scripts are all included in the Github repo, you will have them locally once cloned, thus remove the need to acquire data or use inputs. However, the instructions are listed as though you are creating the project from scratch, and in the case of wanting to do further testing, you can use your own Kaggle API key to test different datasets.

1. Data Acquisition:

â€¢ F-Droid Data:

Run the script to download F-Droid data:

```bash
python3 src/acquire_fdroid.py
```

This will download the raw fdroid.json file into the data/uncleaned/ directory.

â€¢ Google Play Data:

Run the script to download the Google Play Store dataset from Kaggle:

```bash
python3 src/acquire_googleplay.py
```

Make sure you have a valid kaggle.json in the ~/.kaggle/ directory or set the environment variables accordingly.

The raw Google Play data will be saved in data/uncleaned/.

2.  Data Cleaning:

    â€¢ F-Droid Cleaning:

Process the raw JSON to produce cleaned CSV data:

```bash
python3 src/clean_fdroid.py
```

This outputs fdroid_cleaned.csv in data/cleaned/.

â€¢ Google Play Cleaning:

Process the Kaggle CSV dataset:

```bash
python3 src/clean_googleplay.py
```

This outputs googleplay_cleaned.csv in data/cleaned/.

3. Combining Datasets:

Merge both cleaned datasets into one combined dataset with derived features:

```bash
python3 src/combine_datasets.py
```

This creates one file:

â€¢ combined_apps.csv (merged with basic comparative organization)

4. Exploratory Data Analysis (EDA):

Open and run the following Jupyter notebooks for additional exploratory analysis and further feature derivation:

â€¢ Visualization Notebook:

```bash
jupyter notebook notebooks/Visualize_Trends.ipynb
```

This notebook generates visualizations (e.g., distributions, correlations, trends) for further data insights.

5. Feature Engineering:

â€¢ Feature Derivation Notebook:

```bash
jupyter notebook notebooks/Feature_Derivation.ipynb
```

This notebook loads combined_apps_enhanced.csv, derives new features (such as app age, binned installs, and flags), and saves the enhanced dataset.

â€¢ The resulting dataset with new features will be saved for use in modeling.

6. Model Building & Evaluation:

â€¢ Regression Model (Predicting Ratings):

Run the regression script that uses a Random Forest pipeline with GridSearchCV:

```bash
python3 src/build_model.py
```

The script prints best hyperparameters, test set RÂ² and RMSE metrics, and saves a feature importance barplot in the images/ directory.

â€¢ Platform Classification Model:

Run the script to build a Random Forest classifier for predicting the platform (F-Droid vs. Google Play):

```bash
python3 src/platform_classifier_RandomForests.py
```

This outputs a classification report and saves the confusion matrix plot in the images/ directory.

Expected Outputs:

â€¢ Data Files:

- Cleaned data stored in data/cleaned/ and data/combined/ directories.

â€¢ Notebooks:

- Visualizations and derived feature datasets are produced from the notebooks in the notebooks/ folder.

â€¢ Model Metrics and Artifacts:

- Regression model evaluation metrics (RÂ², RMSE) printed to console.
- Feature importances plot saved as images/feature_importances.png.
- Classification report and confusion matrix plot saved as image files in the images/ directory.

Additional Notes:

â€¢ Make sure to update the kaggle.json file in your ~/.kaggle folder if you experience authentication issues with the Kaggle API.

â€¢ You may need to adjust column names or feature lists in the code if the format of the input data changes.

â€¢ All scripts assume that the project is run from the project root directory (i.e., CMPT-353-PROJECT/).

This README provides detailed documentation of code, instructions for running the scripts, dependencies, and the expected file outputs. It ensures reproducibility and helps usersâ€”and evaluatorsâ€”understand the flow of the project and the methods used to meet the assignment requirements.
