# Comparing Proprietary vs. Open-Source Apps: Predicting App Popularity ğŸ“²

## Overview

This project compares apps from the Google Play Store (proprietary) with those from F-Droid (open-source) by predicting app popularity based on ratings, downloads, and other app features. The initial focus is on building a robust pipeline that covers data acquisition, cleaning, exploratory data analysis (EDA), feature engineering, and model building (both classification and regression). Future extensions may include further analysis on privacy aspects if needed.

## Project Structure

```
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ google_play.csv # Kaggle dataset for Google Play apps
â”‚ â””â”€â”€ fdroid.json # JSON data scraped/collected from F-Droid
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 1_EDA.ipynb # Jupyter notebook for exploratory data analysis
â”‚ â”œâ”€â”€ 2_FeatureEngineering.ipynb # Notebook for creating new features
â”‚ â””â”€â”€ 3_ModelBuilding.ipynb # Notebook for training and evaluating models
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_cleaning.py # Script to clean and preprocess raw data
â”‚ â”œâ”€â”€ feature_engineering.py # Script to create and transform features
â”‚ â”œâ”€â”€ model_training.py # Script for model training and evaluation
â”‚ â””â”€â”€ utils.py # Helper functions
â”œâ”€â”€ requirements.txt # List of required libraries
â””â”€â”€ README.md
```

## Required Libraries

The project is implemented in Python and requires the following libraries:

- **pandas** â€“ for data manipulation and cleaning
- **numpy** â€“ for numerical operations
- **scikit-learn** â€“ for model building, evaluation, and pipeline management
- **matplotlib** and **seaborn** â€“ for visualization
- **requests** â€“ for data collection (if scraping F-Droid data)
- **json** â€“ for parsing JSON data

Install the libraries using:

```bash
pip install -r requirements.txt
```

## How to Run the Project

1. Data Acquisition & Cleaning:
   â€¢ Ensure that the raw data files (google_play.csv and fdroid.json) are placed in the data/ directory.
   â€¢ Run the data cleaning script:

```bash
python3 src/data_cleaning.py
```

This will process the raw data and output cleaned data files in a designated folder (e.g., data/cleaned/).

2. Exploratory Data Analysis (EDA):

   â€¢ Open the notebook notebooks/1_EDA.ipynb in Jupyter Notebook or JupyterLab.

   â€¢ Follow the cells to explore the data and generate visualizations (e.g., histograms, box plots, heatmaps).

3. Feature Engineering:

   â€¢ Open notebooks/2_FeatureEngineering.ipynb and run the cells to create new features such as Price Tier, Size Category, and Update Frequency.

   â€¢ The resulting dataset with new features will be saved for use in modeling.

4. Model Building & Evaluation:

   â€¢ Open notebooks/3_ModelBuilding.ipynb to train models.

   â€¢ For the classification task, the model predicts whether an app will have a â€œhighâ€ rating (e.g., â‰¥ 4.0).

   â€¢ For the regression task, the model predicts the exact rating or number of installs.

   â€¢ Models are built using scikit-learn pipelines; results are displayed using model metrics (F1-score, RMSE, etc.).

Files Produced / Expected Outputs:

â€¢ Cleaned Data Files: Processed CSV/JSON files in data/cleaned/.

â€¢ EDA Visualizations: Figures (histograms, scatter plots, heatmaps) generated in the EDA notebook.

â€¢ Engineered Dataset: A dataset with additional features saved for modeling.

â€¢ Model Metrics: Output scores (accuracy, F1, RMSE, etc.) displayed in the Model Building notebook.

â€¢ Final Models: Saved models or pipeline objects (if applicable) in a designated models folder (optional).
